{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde2f3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people/images/WechatIMG199.jpg', 'people/images/WechatIMG200.jpg', 'people/images/WechatIMG201.jpg', 'people/images/WechatIMG202.jpg', 'people/images/WechatIMG203.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from vggt.models.vggt import VGGT\n",
    "\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "# Initialize the model and load the pretrained weights.\n",
    "# This will automatically download the model weights the first time it's run, which may take a while.\n",
    "model = VGGT.from_pretrained(\"facebook/VGGT-1B\").to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e720117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people/images/WechatIMG199.jpg', 'people/images/WechatIMG200.jpg', 'people/images/WechatIMG201.jpg', 'people/images/WechatIMG202.jpg', 'people/images/WechatIMG203.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Read images from 'people/images' directory\n",
    "# image_dir = 'examples/room/images'\n",
    "image_dir = 'people/images'\n",
    "image_names = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "print(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f37b7064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people/images/WechatIMG200.jpg', 'people/images/WechatIMG201.jpg', 'people/images/WechatIMG202.jpg', 'people/images/WechatIMG203.jpg', 'people/images/WechatIMG199.jpg']\n"
     ]
    }
   ],
   "source": [
    "# reorder the fourth image to be the first\n",
    "index_to_move = 0\n",
    "image_names.insert(4, image_names.pop(index_to_move))\n",
    "print(image_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1677768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "\n",
    "images = load_and_preprocess_images(image_names, mode='pad').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=dtype):\n",
    "        images = images[None]  # add batch dimension\n",
    "        aggregated_tokens_list, ps_idx = model.aggregator(images)\n",
    "                \n",
    "    # Predict Cameras\n",
    "    pose_enc = model.camera_head(aggregated_tokens_list)[-1]\n",
    "    # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)\n",
    "    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])\n",
    "\n",
    "    # Predict Depth Maps\n",
    "    depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)\n",
    "\n",
    "    # Predict Point Maps\n",
    "    point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)\n",
    "        \n",
    "    # Construct 3D Points from Depth Maps and Cameras\n",
    "    # which usually leads to more accurate 3D points than point map branch\n",
    "    point_map_by_unprojection = unproject_depth_map_to_point_map(depth_map.squeeze(0), \n",
    "                                                                extrinsic.squeeze(0), \n",
    "                                                                intrinsic.squeeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de62eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Predict Tracks\n",
    "    # choose your own points to track, with shape (N, 2) for one scene\n",
    "    query_points = torch.FloatTensor([[224.4112, 187.6958],\n",
    "          [251.5651, 189.3672]]).to(device)\n",
    "    track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "612680b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/tracks_grid.png\n",
      "[INFO] Saved 5 individual frames to track_visuals/frame_*.png\n"
     ]
    }
   ],
   "source": [
    "from vggt.utils.visual_track import visualize_tracks_on_images\n",
    "track = track_list[-1]\n",
    "visualize_tracks_on_images(images, track, (conf_score>0.2) & (vis_score>0.2), out_dir=\"track_visuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99643df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[172.0000, 172.0000],\n",
       "          [340.0000, 180.9400]],\n",
       "\n",
       "         [[224.4112, 187.6958],\n",
       "          [251.5651, 189.3672]],\n",
       "\n",
       "         [[282.4581, 151.3021],\n",
       "          [244.2977, 155.3082]],\n",
       "\n",
       "         [[221.4864, 449.5332],\n",
       "          [269.3793, 388.7892]],\n",
       "\n",
       "         [[413.5035, 145.3219],\n",
       "          [440.6994, 243.9181]]]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
